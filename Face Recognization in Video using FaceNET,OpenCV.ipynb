{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognize in Video using FaceNET, OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pickle\n",
    "import imutils\n",
    "import argparse\n",
    "import numpy as np\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap=argparse.ArgumentParser()\n",
    "\n",
    "ap.add_argument(\"--video\",required=True,help=\"input video path\")\n",
    "ap.add_argument(\"--faceModel\",required=True,help=\"path for face detection model\")\n",
    "ap.add_argument(\"--prototxt\",required=True,help=\"path of prototxt model\")\n",
    "ap.add_argument(\"--embeddingModel\",required=True,help=\"path of embedding model\")\n",
    "ap.add_argument(\"--recognizer\",required=True,help=\"path of train SVM model\")\n",
    "ap.add_argument(\"--le\",required=True,help=\"path to label encoder\")\n",
    "\n",
    "args=vars(ap.parse_args([\"--video\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\output\\4.mp4\",\n",
    "                        \"--faceModel\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\pre\\face_detection_model\\res10_300x300_ssd_iter_140000.caffemodel\",\n",
    "                        \"--prototxt\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\pre\\face_detection_model\\deploy.prototxt\",\n",
    "                        \"--embeddingModel\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\pre\\openface_nn4.small2.v1.t7\",\n",
    "                        \"--recognizer\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\output\\Test\\recognizer2.h5\",\n",
    "                        \"--le\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\output\\label.pickle\"\n",
    "                        ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading face detection model\n",
    "face_detector=cv2.dnn.readNetFromCaffe(args[\"prototxt\"],args[\"faceModel\"])\n",
    "\n",
    "#loading face embedding model\n",
    "face_embedder=cv2.dnn.readNetFromTorch(args[\"embeddingModel\"])\n",
    "\n",
    "#loading the actual face recognition model \n",
    "face_recognizer=load_model(args[\"recognizer\"])\n",
    "\n",
    "#label encoder loading\n",
    "le=pickle.loads(open(args[\"le\"],\"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Video Stream ,and detect by frame by frame in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video=cv2.VideoCapture(r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\output\\4.mp4\")\n",
    "#video=VideoStream(args[\"video\"]).start()\n",
    "#fps=FPS().start()\n",
    "#loop over frame by frame\n",
    "while True:\n",
    "    ret,frame=video.read()\n",
    "    \n",
    "    #resize the frame and then grab the image dimenssions\n",
    "    frame=cv2.resize(frame,(650,600))\n",
    "    (h,w)=frame.shape[:2]\n",
    "    \n",
    "    #construct the blob from image\n",
    "    imageBlob=cv2.dnn.blobFromImage(frame,1.0,(300,300),(104.0,177.0,123.0),swapRB=False,crop=False)\n",
    "    \n",
    "    #apply OpenCV deep learning based face detector to localize faces in the input image\n",
    "    face_detector.setInput(imageBlob)\n",
    "    detections=face_detector.forward()\n",
    "    \n",
    "    #loop over the detections\n",
    "    for i in range(0,detections.shape[2]):\n",
    "        #extract the confidence (i.e probability) associate with the predictions\n",
    "        confidence=detections[0,0,i,2]\n",
    "        \n",
    "        #filter out the weak detections\n",
    "        if confidence>0.5:\n",
    "            #compute the (x,y) coordinate of the bounding box of the faces\n",
    "            box=detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "            (start_x,start_y,end_x,end_y)=box.astype(\"int\")\n",
    "            \n",
    "            #extract the face ROI\n",
    "            face=frame[start_y:end_y,start_x:end_x]\n",
    "            (fH,fW)=face.shape[:2]\n",
    "            \n",
    "            #ensure face width & height are sufficently large\n",
    "            if fW<20 or fH<20:\n",
    "                continue\n",
    "            \n",
    "            #construct the blob for the face ROI , then pass the blob through our face embedding model\n",
    "            #to obtain the 128-d quantifications of faces \n",
    "            faceBlob=cv2.dnn.blobFromImage(face,1.0/255,(96,96),(0,0,0),swapRB=True,crop=False)\n",
    "            face_embedder.setInput(faceBlob)\n",
    "            vec=face_embedder.forward()\n",
    "            \n",
    "            #perform the classifications to recognize the face\n",
    "            preds=face_recognizer.predict_proba(vec)[0]\n",
    "            j=np.argmax(preds)\n",
    "            proba=preds[j]\n",
    "            name=le.classes_[j]\n",
    "            \n",
    "            #draw the bounding box of the face along the associate probability\n",
    "            text=\"{} : {:.2f}%\".format(name,proba*100)\n",
    "            y=start_y-10 if start_y-10>10 else start_y+10\n",
    "            cv2.rectangle(frame,(start_x,start_y),(end_x,end_y),(0,0,255),2)\n",
    "            cv2.putText(frame,text,(start_x,y),cv2.FONT_HERSHEY_SIMPLEX,0.45,(0,0,255),2)\n",
    "           \n",
    "    #fps.update()\n",
    "    cv2.imshow(\"Frame\",frame)\n",
    "    key=cv2.waitKey(1) &0xFF\n",
    "    if key==ord(\"q\"):\n",
    "        break\n",
    "#fps.stop()\n",
    "cv2.destroyAllWindows()\n",
    "video.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
