{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Using Facenet,Deep Learning &OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Embeddings from Face Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the argument and parse the argumens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap=argparse.ArgumentParser()\n",
    "\n",
    "ap.add_argument(\"--dataset\",required=True,help=\"path of dataset\")\n",
    "ap.add_argument(\"--embeddings\",required=True,help=\"path of output data of facial embeddings\")\n",
    "ap.add_argument(\"--faceModel\",required=True,help=\"path for face detection model\")\n",
    "ap.add_argument(\"--prototxt\",required=True,help=\"path of prototxt model\")\n",
    "ap.add_argument(\"--embeddingModel\",required=True,help=\"path of embedding model\")\n",
    "\n",
    "args=vars(ap.parse_args([\"--dataset\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\dataset\\Test\",\n",
    "                        \"--embeddings\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\output\",\n",
    "                        \"--faceModel\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\pre\\face_detection_model\\res10_300x300_ssd_iter_140000.caffemodel\",\n",
    "                        \"--prototxt\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\pre\\face_detection_model\\deploy.prototxt\",\n",
    "                        \"--embeddingModel\",r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\pre\\openface_nn4.small2.v1.t7\",\n",
    "                        ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading face_detection model\n",
    "#Caffe based DL face detector to localize faces in an image.\n",
    "face_detector=cv2.dnn.readNetFromCaffe(args[\"prototxt\"],args[\"faceModel\"])\n",
    "\n",
    "#loading face Embedding model\n",
    "#This model is Torch-based and is responsible for extracting facial embeddings via deep learning feature extraction.\n",
    "face_embedder=cv2.dnn.readNetFromTorch(args[\"embeddingModel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths=list(paths.list_images(args[\"dataset\"]))\n",
    "\n",
    "#initialize our lists of extracted facial embedding & corresponidng name\n",
    "knownEmbeddings=[]\n",
    "knownNames=[]\n",
    "\n",
    "#initialized the total no of faces processed \n",
    "total=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing image 1/250\n",
      "processing image 2/250\n",
      "processing image 3/250\n",
      "processing image 4/250\n",
      "processing image 5/250\n",
      "processing image 6/250\n",
      "processing image 7/250\n",
      "processing image 8/250\n",
      "processing image 9/250\n",
      "processing image 10/250\n",
      "processing image 11/250\n",
      "processing image 12/250\n",
      "processing image 13/250\n",
      "processing image 14/250\n",
      "processing image 15/250\n",
      "processing image 16/250\n",
      "processing image 17/250\n",
      "processing image 18/250\n",
      "processing image 19/250\n",
      "processing image 20/250\n",
      "processing image 21/250\n",
      "processing image 22/250\n",
      "processing image 23/250\n",
      "processing image 24/250\n",
      "processing image 25/250\n",
      "processing image 26/250\n",
      "processing image 27/250\n",
      "processing image 28/250\n",
      "processing image 29/250\n",
      "processing image 30/250\n",
      "processing image 31/250\n",
      "processing image 32/250\n",
      "processing image 33/250\n",
      "processing image 34/250\n",
      "processing image 35/250\n",
      "processing image 36/250\n",
      "processing image 37/250\n",
      "processing image 38/250\n",
      "processing image 39/250\n",
      "processing image 40/250\n",
      "processing image 41/250\n",
      "processing image 42/250\n",
      "processing image 43/250\n",
      "processing image 44/250\n",
      "processing image 45/250\n",
      "processing image 46/250\n",
      "processing image 47/250\n",
      "processing image 48/250\n",
      "processing image 49/250\n",
      "processing image 50/250\n",
      "processing image 51/250\n",
      "processing image 52/250\n",
      "processing image 53/250\n",
      "processing image 54/250\n",
      "processing image 55/250\n",
      "processing image 56/250\n",
      "processing image 57/250\n",
      "processing image 58/250\n",
      "processing image 59/250\n",
      "processing image 60/250\n",
      "processing image 61/250\n",
      "processing image 62/250\n",
      "processing image 63/250\n",
      "processing image 64/250\n",
      "processing image 65/250\n",
      "processing image 66/250\n",
      "processing image 67/250\n",
      "processing image 68/250\n",
      "processing image 69/250\n",
      "processing image 70/250\n",
      "processing image 71/250\n",
      "processing image 72/250\n",
      "processing image 73/250\n",
      "processing image 74/250\n",
      "processing image 75/250\n",
      "processing image 76/250\n",
      "processing image 77/250\n",
      "processing image 78/250\n",
      "processing image 79/250\n",
      "processing image 80/250\n",
      "processing image 81/250\n",
      "processing image 82/250\n",
      "processing image 83/250\n",
      "processing image 84/250\n",
      "processing image 85/250\n",
      "processing image 86/250\n",
      "processing image 87/250\n",
      "processing image 88/250\n",
      "processing image 89/250\n",
      "processing image 90/250\n",
      "processing image 91/250\n",
      "processing image 92/250\n",
      "processing image 93/250\n",
      "processing image 94/250\n",
      "processing image 95/250\n",
      "processing image 96/250\n",
      "processing image 97/250\n",
      "processing image 98/250\n",
      "processing image 99/250\n",
      "processing image 100/250\n",
      "processing image 101/250\n",
      "processing image 102/250\n",
      "processing image 103/250\n",
      "processing image 104/250\n",
      "processing image 105/250\n",
      "processing image 106/250\n",
      "processing image 107/250\n",
      "processing image 108/250\n",
      "processing image 109/250\n",
      "processing image 110/250\n",
      "processing image 111/250\n",
      "processing image 112/250\n",
      "processing image 113/250\n",
      "processing image 114/250\n",
      "processing image 115/250\n",
      "processing image 116/250\n",
      "processing image 117/250\n",
      "processing image 118/250\n",
      "processing image 119/250\n",
      "processing image 120/250\n",
      "processing image 121/250\n",
      "processing image 122/250\n",
      "processing image 123/250\n",
      "processing image 124/250\n",
      "processing image 125/250\n",
      "processing image 126/250\n",
      "processing image 127/250\n",
      "processing image 128/250\n",
      "processing image 129/250\n",
      "processing image 130/250\n",
      "processing image 131/250\n",
      "processing image 132/250\n",
      "processing image 133/250\n",
      "processing image 134/250\n",
      "processing image 135/250\n",
      "processing image 136/250\n",
      "processing image 137/250\n",
      "processing image 138/250\n",
      "processing image 139/250\n",
      "processing image 140/250\n",
      "processing image 141/250\n",
      "processing image 142/250\n",
      "processing image 143/250\n",
      "processing image 144/250\n",
      "processing image 145/250\n",
      "processing image 146/250\n",
      "processing image 147/250\n",
      "processing image 148/250\n",
      "processing image 149/250\n",
      "processing image 150/250\n",
      "processing image 151/250\n",
      "processing image 152/250\n",
      "processing image 153/250\n",
      "processing image 154/250\n",
      "processing image 155/250\n",
      "processing image 156/250\n",
      "processing image 157/250\n",
      "processing image 158/250\n",
      "processing image 159/250\n",
      "processing image 160/250\n",
      "processing image 161/250\n",
      "processing image 162/250\n",
      "processing image 163/250\n",
      "processing image 164/250\n",
      "processing image 165/250\n",
      "processing image 166/250\n",
      "processing image 167/250\n",
      "processing image 168/250\n",
      "processing image 169/250\n",
      "processing image 170/250\n",
      "processing image 171/250\n",
      "processing image 172/250\n",
      "processing image 173/250\n",
      "processing image 174/250\n",
      "processing image 175/250\n",
      "processing image 176/250\n",
      "processing image 177/250\n",
      "processing image 178/250\n",
      "processing image 179/250\n",
      "processing image 180/250\n",
      "processing image 181/250\n",
      "processing image 182/250\n",
      "processing image 183/250\n",
      "processing image 184/250\n",
      "processing image 185/250\n",
      "processing image 186/250\n",
      "processing image 187/250\n",
      "processing image 188/250\n",
      "processing image 189/250\n",
      "processing image 190/250\n",
      "processing image 191/250\n",
      "processing image 192/250\n",
      "processing image 193/250\n",
      "processing image 194/250\n",
      "processing image 195/250\n",
      "processing image 196/250\n",
      "processing image 197/250\n",
      "processing image 198/250\n",
      "processing image 199/250\n",
      "processing image 200/250\n",
      "processing image 201/250\n",
      "processing image 202/250\n",
      "processing image 203/250\n",
      "processing image 204/250\n",
      "processing image 205/250\n",
      "processing image 206/250\n",
      "processing image 207/250\n",
      "processing image 208/250\n",
      "processing image 209/250\n",
      "processing image 210/250\n",
      "processing image 211/250\n",
      "processing image 212/250\n",
      "processing image 213/250\n",
      "processing image 214/250\n",
      "processing image 215/250\n",
      "processing image 216/250\n",
      "processing image 217/250\n",
      "processing image 218/250\n",
      "processing image 219/250\n",
      "processing image 220/250\n",
      "processing image 221/250\n",
      "processing image 222/250\n",
      "processing image 223/250\n",
      "processing image 224/250\n",
      "processing image 225/250\n",
      "processing image 226/250\n",
      "processing image 227/250\n",
      "processing image 228/250\n",
      "processing image 229/250\n",
      "processing image 230/250\n",
      "processing image 231/250\n",
      "processing image 232/250\n",
      "processing image 233/250\n",
      "processing image 234/250\n",
      "processing image 235/250\n",
      "processing image 236/250\n",
      "processing image 237/250\n",
      "processing image 238/250\n",
      "processing image 239/250\n",
      "processing image 240/250\n",
      "processing image 241/250\n",
      "processing image 242/250\n",
      "processing image 243/250\n",
      "processing image 244/250\n",
      "processing image 245/250\n",
      "processing image 246/250\n",
      "processing image 247/250\n",
      "processing image 248/250\n",
      "processing image 249/250\n",
      "processing image 250/250\n"
     ]
    }
   ],
   "source": [
    "#loop over the image paths\n",
    "for (i,imagePath) in enumerate(imagePaths):\n",
    "    \n",
    "    #extract person name from image path\n",
    "    print(\"processing image {}/{}\".format(i+1,len(imagePaths)))\n",
    "    name=imagePath.split(os.path.sep)[-2]\n",
    "    \n",
    "    #load the image , resize image and grab the image dimensions\n",
    "    image=cv2.imread(imagePath)\n",
    "    image=cv2.resize(image,(650,600))\n",
    "    (h,w)=image.shape[:2]\n",
    "    \n",
    "    #construct a blob from the image\n",
    "    imageBlob=cv2.dnn.blobFromImage(image,1.0,(300,300),(104.0,177.0,123.0),swapRB=False,crop=False)\n",
    "    \n",
    "    #apply opencv deep learning-based face detector to localize faces in the input image\n",
    "    face_detector.setInput(imageBlob)\n",
    "    detections=face_detector.forward()\n",
    "    \n",
    "    #ensure atlest one face is detected \n",
    "    if len(detections)>0:\n",
    "        #making assumption that each image has only one face , so find bounding box with largest probability\n",
    "        i=np.argmax(detections[0,0,:,2])\n",
    "        confidence=detections[0,0,i,2]\n",
    "        \n",
    "        #ensure that the detections with largests probability also have min >0.5\n",
    "        if confidence>0.5:\n",
    "            box=detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "            (start_x,start_y,end_x,end_y)=box.astype(\"int\")\n",
    "            \n",
    "            #extract the face ROI and grab the ROI dimensions\n",
    "            face=image[start_y:end_y,start_x:end_x]\n",
    "            (fH,fW)=face.shape[:2]\n",
    "            \n",
    "            #ensure that face width & height are sufficiently large\n",
    "            if fW<20 or fH<20:\n",
    "                continue\n",
    "            \n",
    "            #construct the blob for the face ROI , then pass the blob through our face embedding model to\n",
    "            #obtained the 128-d quantification of the face\n",
    "            faceBlob=cv2.dnn.blobFromImage(face,1.0/255,(96,96),(0,0,0),swapRB=True,crop=False)\n",
    "            face_embedder.setInput(faceBlob)\n",
    "            vec=face_embedder.forward()\n",
    "            \n",
    "            #add the name of the person & corresponding face embedding to their respective list\n",
    "            knownNames.append(name)\n",
    "            knownEmbeddings.append(vec.flatten())\n",
    "            total +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the facial embedding & name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\"embedding\":knownEmbeddings,\"names\":knownNames}\n",
    "f=open(r\"C:\\Users\\SRKT\\Desktop\\opencv-face-recognition\\output\\Test\\embeddings.pickle\",\"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(knownEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
